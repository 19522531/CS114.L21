{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Craw_data_bonus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i--gFFdAzWAA"
      },
      "source": [
        "#TheDailyMash.co.uk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9Z8FwO0qBYE"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--7eVPRqGMj",
        "outputId": "2aa7c36a-4424-4906-c7f7-f619cda25b91"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "\n",
        "def getArticleFromPage(page):\n",
        "    API_url = \"https://www.thedailymash.co.uk/politics?page={}\".format(str(page))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"a\", class_=\"font-serif font-bold text-xl xl:text-2xl text-brand hover:underline leading-none xl:leading-tight\")\n",
        "    print(response)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        headline = x.get_text()\n",
        "        article_link = \"https://www.thedailymash.co.uk\" + x['href']\n",
        "        is_sarcastic = '1'\n",
        "        result.append([headline, article_link, is_sarcastic])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = __file__.replace(\".py\", \".txt\")\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = i[0] + \"|\" + i[1] + \"|\" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "    print(headlineData)\n",
        "        \n",
        "def getDataFromPage(page):\n",
        "    print(\"=>>>>> Page: {}\".format(page)) \n",
        "    soupArticle = getArticleFromPage(page)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    print(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    page = 1\n",
        "    headline = []\n",
        "    article_link = []\n",
        "    is_sarcastic = []\n",
        "    while page < 32: #03/06/2021\n",
        "      print(\"=>>>>> Page: {}\".format(page)) \n",
        "      soupArticle = getArticleFromPage(page)\n",
        "      headlineData = getHeadlineDataFormArticle(soupArticle)      \n",
        "      for i in headlineData:\n",
        "        headline.append(i[0])\n",
        "        article_link.append(i[1])\n",
        "        is_sarcastic.append(i[2])\n",
        "      print(\"Number of data: \",len(headlineData))\n",
        "      print(\"\\n\")\n",
        "      page += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=>>>>> Page: 1\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 2\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 3\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 4\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 5\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 6\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 7\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 8\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 9\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 10\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 11\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 12\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 13\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 14\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 15\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 16\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 17\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 18\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 19\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 20\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 21\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 22\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 23\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 24\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 25\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 26\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 27\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 28\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 29\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 30\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n",
            "=>>>>> Page: 31\n",
            "<Response [200]>\n",
            "Number of data:  10\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWv-bVYywXfF"
      },
      "source": [
        "import pandas as pd\n",
        "data = {}\n",
        "data[\"headline\"] = pd.Series(headline)\n",
        "data['article_link'] = pd.Series(article_link)\n",
        "data['is_sarcastic'] = pd.Series(is_sarcastic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLQetYzzxEJR"
      },
      "source": [
        "data = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUlcMCyQxQyX"
      },
      "source": [
        "data.to_csv(\"thedailymash.co.uk_1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Tydj9Qzcnf"
      },
      "source": [
        "#rochdaleherald.co.uk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJt7dWmbzdYD"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import json\n",
        "\n",
        "def getArticleFromPage(day, month, year):\n",
        "    API_url = \"https://rochdaleherald.co.uk/{}/{}/{}\".format(str(year), str(month),str(day))\n",
        "    print(API_url)\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"h3\", class_=\"entry-title td-module-title\")\n",
        "    print(response)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        tmp = x.find(\"a\")\n",
        "        headline = tmp['title']\n",
        "        article_link = tmp['href']\n",
        "        article_link = article_link[:-1]\n",
        "        is_sarcastic = '1'\n",
        "        result.append([headline, article_link, is_sarcastic])\n",
        "    return result\n",
        "\n",
        "# def writeFile(headlineData):\n",
        "#     fileOutput = __file__.replace(\".py\", \".txt\")\n",
        "#     f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "#     for i in headlineData:\n",
        "#         strOut = i[0] + \"|\" + i[1] + \"|\" + i[2] + \"\\n\"\n",
        "#         f.writelines(strOut)\n",
        "#     f.close()\n",
        "        \n",
        "def getDataFromPage(day, month, year):\n",
        "    print(\"=>>>>> {}/{}/{}:\".format(day,month, year)) \n",
        "    soupArticle = getArticleFromPage(day,month, year)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    #writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "    return headlineData\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = 2021\n",
        "    year_stop = 2015\n",
        "    headline = []\n",
        "    article_link = []\n",
        "    is_sarcastic = []\n",
        "    while year > year_stop - 1:\n",
        "        if year == 2021:\n",
        "            month = 6\n",
        "        else:\n",
        "            month = 12\n",
        "        while month:\n",
        "            if month in [1,2,3,4,5,6,7,8,9]:\n",
        "                monthh = '0' + str(month)\n",
        "            else:\n",
        "                monthh = month\n",
        "            day = 31\n",
        "            while day:\n",
        "                if day in [1,2,3,4,5,6,7,8,9]:\n",
        "                    dayy =  '0' + str(day)\n",
        "                else:\n",
        "                    dayy = day\n",
        "                lst = getDataFromPage(dayy, monthh, year)\n",
        "                for i in lst:\n",
        "                  headline.append(i[0])\n",
        "                  article_link.append(i[1])\n",
        "                  is_sarcastic.append(i[2])\n",
        "                day -= 1\n",
        "            month -= 1\n",
        "        year -= 1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLtuO5P01w11",
        "outputId": "153ae1cc-5a3e-450b-dd6a-e9bfe98f91c6"
      },
      "source": [
        "print(len(headline))\n",
        "print(len(article_link))\n",
        "print(len(is_sarcastic))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14398\n",
            "14398\n",
            "14398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Lkk6aR4lHb"
      },
      "source": [
        "import pandas as pd\n",
        "data = {}\n",
        "data[\"headline\"] = pd.Series(headline)\n",
        "data['article_link'] = pd.Series(article_link)\n",
        "data['is_sarcastic'] = pd.Series(is_sarcastic)\n",
        "data = pd.DataFrame(data)\n",
        "data.to_csv(\"rochdaleherald.co.uk_1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sC8j9F3rRF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}